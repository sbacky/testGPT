# testGPT

The purpose of this project is to learn how to build a Generatively Pretrained Transformer (GPT), following the paper "Attention is All You Need" and OpenAI's GPT-2 / GPT-3. I train a decoder only transformer on the tiny shakespeare dataset to replicate shakespeare dialogue.

All code is in 'bigram.py' and additional documentation is available in the jupyter notebook 'gpt-dev.ipynb'. The original step by step information by Andrej Karpathy is available [here](https://www.youtube.com/watch?v=kCc8FmEb1nY).
